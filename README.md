# Apprentissage-par-Renforcement-avec-l-quation-de-Bellman-sur-GridWorld


## Description

Ce programme impl√©mente un algorithme d'it√©ration sur les valeurs pour r√©soudre un probl√®me de planification de politique dans une grille. Le but est de trouver la politique optimale pour un agent qui se d√©place dans une grille, en maximisant la r√©compense totale en suivant les meilleures actions possibles.

Le programme prend en compte diff√©rents √©l√©ments de la grille, tels que les √©tats cibles (reward), les dangers (penalty), et les cellules bloqu√©es. L'algorithme calcule les valeurs des √©tats et extrait la politique optimale en fonction de ces valeurs.

## Configuration

La grille est d√©finie par ses dimensions (3x4) avec les √©l√©ments suivants :

- **Cellule cible (G)** : La cellule (0, 3) dans la grille, o√π une r√©compense de 1 est attribu√©e.
- **Cellule dangereuse (üî•)** : La cellule (1, 3) dans la grille, o√π une p√©nalit√© de -1 est attribu√©e.
- **Cellule bloqu√©e (‚ñà)** : La cellule (1, 1) dans la grille, qui est inaccessible.
- **R√©compense par pas** : Chaque mouvement dans la grille donne une r√©compense de 0 (c'est-√†-dire qu'il n'y a pas de co√ªt par mouvement).

Les mouvements disponibles pour l'agent sont :
- Haut (‚Üë)
- Bas (‚Üì)
- Gauche (‚Üê)
- Droite (‚Üí)

## Fonctionnement du Code

### 1. **`compute_values`** : Calcul des valeurs des √©tats

Cette fonction effectue l'it√©ration sur les valeurs pour calculer les valeurs des √©tats dans la grille. √Ä chaque it√©ration, elle met √† jour les valeurs des √©tats en fonction des r√©compenses actuelles et des valeurs estim√©es des voisins, en utilisant un facteur de r√©duction (`discount_factor`) pour pond√©rer les r√©compenses futures.

#### Processus :
- La fonction commence par une copie des valeurs des √©tats actuels.
- Pour chaque √©tat (cellule de la grille), elle √©value les valeurs possibles des √©tats voisins dans les quatre directions (haut, bas, gauche, droite).
- Si un √©tat voisin est valide (dans les limites de la grille et non bloqu√©), sa valeur est calcul√©e comme √©tant la somme de la r√©compense actuelle de cet √©tat et du produit du facteur de r√©duction et de la valeur estim√©e de cet √©tat voisin.
- La fonction met √† jour les valeurs des √©tats √† chaque it√©ration jusqu'√† ce que le nombre maximal d'it√©rations soit atteint.

### 2. **`derive_policy`** : Extraction de la politique optimale

Cette fonction d√©rive la politique optimale en choisissant pour chaque √©tat l'action qui conduit √† l'√©tat voisin ayant la valeur la plus √©lev√©e.

#### Processus :
- Pour chaque cellule de la grille, la fonction √©value les quatre directions possibles (haut, bas, gauche, droite).
- Elle choisit la direction qui maximise la somme de la r√©compense actuelle et de la valeur estim√©e de l'√©tat voisin.
- Si un √©tat est terminal (cible, danger, ou bloqu√©), il re√ßoit un symbole sp√©cifique (G, üî•, ou ‚ñà).

La politique optimale est ensuite retourn√©e sous forme d'une grille de symboles repr√©sentant les mouvements √† effectuer pour maximiser les r√©compenses.

### 3. **Affichage des r√©sultats**

Une fois les valeurs des √©tats calcul√©es et la politique optimale d√©riv√©e, le programme affiche :
- La politique optimale sous forme de symboles : 
  - `G` pour la cellule cible
  - `üî•` pour la cellule dangereuse
  - `‚ñà` pour la cellule bloqu√©e
  - `‚Üë`, `‚Üì`, `‚Üê`, `‚Üí` pour les directions optimales
- Les valeurs des √©tats (r√©compenses estim√©es) sous forme de matrice.

## Exemple d'Ex√©cution

Voici un exemple de ce √† quoi ressemble l'affichage de la politique optimale :

![image](https://github.com/user-attachments/assets/5b11c550-f1bf-41f6-8ef0-39a1bb1c0498)
